# üöÄ Guide d'Installation - Assistant RAG INPT

## Installation Rapide (5 minutes)

### Pr√©requis
- Python 3.11+ install√©
- 8GB RAM minimum
- 10GB espace disque libre

### √âtapes d'Installation

#### 1. Pr√©paration de l'Environnement
```bash
# Naviguer vers le projet
cd inpt-rag-assistant

# Cr√©er l'environnement virtuel
python3 -m venv venv

# Activer l'environnement
source venv/bin/activate  # macOS/Linux
# ou
venv\Scripts\activate     # Windows
```

#### 2. Installation des D√©pendances
```bash
# Installer les packages Python
pip install -r requirements.txt

# T√©l√©charger les donn√©es NLTK
python -c "import nltk; nltk.download('punkt')"
```

#### 3. Installation d'Ollama
```bash
# Installer Ollama (macOS/Linux)
curl -fsSL https://ollama.ai/install.sh | sh

# D√©marrer le service
ollama serve &

# T√©l√©charger le mod√®le LLM
ollama pull llama3.2:3b
```

#### 4. Initialisation du Syst√®me
```bash
# Cr√©er les bases de donn√©es
python scripts/setup_database.py

# Configurer l'environnement
cp .env.example .env
```

#### 5. Test de l'Installation
```bash
# Lancer l'application
streamlit run app/streamlit_app.py
```

L'application s'ouvre automatiquement sur `http://localhost:8501`

## Installation avec Docker (Alternative)

Si vous pr√©f√©rez Docker :

```bash
# Naviguer vers le dossier Docker
cd docker

# Lancer tous les services
docker-compose up -d

# Acc√©der √† l'application
open http://localhost:8501
```

## Premier Usage

### 1. Ajouter des Documents
```bash
# Copier vos documents PDF/TXT dans le dossier
cp ~/mes-cours/*.pdf data/documents/

# Ing√©rer les documents
python scripts/ingest_documents.py data/documents --recursive
```

### 2. Tester le Syst√®me
1. Aller sur `http://localhost:8501`
2. Naviguer vers la page "üí¨ Chat"
3. Poser une question : "Qu'est-ce que le machine learning ?"

## R√©solution de Probl√®mes

### Erreur "Module not found"
```bash
# V√©rifier l'environnement virtuel
source venv/bin/activate
pip install -r requirements.txt
```

### Erreur "Ollama connection"
```bash
# V√©rifier qu'Ollama fonctionne
curl http://localhost:11434/api/tags

# Si pas de r√©ponse, red√©marrer
ollama serve
```

### Erreur "Out of memory"
```bash
# Utiliser un mod√®le plus l√©ger
ollama pull llama3.2:1b

# Modifier .env
echo "OLLAMA_MODEL=llama3.2:1b" >> .env
```

## Configuration Personnalis√©e

√âditez le fichier `.env` pour personnaliser :

```bash
# Mod√®le LLM (choisir selon vos ressources)
OLLAMA_MODEL="llama3.2:3b"  # Recommand√©
# OLLAMA_MODEL="llama3.2:1b"  # Plus l√©ger
# OLLAMA_MODEL="mistral:7b"   # Plus performant

# Param√®tres de recherche
TOP_K_RETRIEVAL=10
SEMANTIC_WEIGHT=0.7
BM25_WEIGHT=0.3

# Param√®tres de g√©n√©ration
LLM_TEMPERATURE=0.1
LLM_MAX_TOKENS=500
```

## V√©rification de l'Installation

### Commandes de Test
```bash
# V√©rifier Python
python --version  # Doit √™tre 3.11+

# V√©rifier les packages
pip list | grep streamlit
pip list | grep chromadb

# V√©rifier Ollama
ollama list

# V√©rifier la base de donn√©es
python scripts/ingest_documents.py --stats
```

### Indicateurs de Succ√®s
- ‚úÖ Streamlit d√©marre sans erreur
- ‚úÖ Page web accessible sur localhost:8501
- ‚úÖ Ollama r√©pond aux requ√™tes
- ‚úÖ Base de donn√©es initialis√©e
- ‚úÖ Documents ing√©r√©s avec succ√®s

## Support

En cas de probl√®me :
1. V√©rifiez les logs dans `logs/`
2. Consultez `GUIDE_UTILISATEUR.md`
3. Red√©marrez tous les services
4. R√©initialisez la base de donn√©es si n√©cessaire

---

**Installation termin√©e !** Vous pouvez maintenant utiliser votre assistant RAG. üéâ
"""
Script d'ingestion de documents dans le syst√®me RAG
Traite PDF, TXT, MD, DOCX et les indexe dans ChromaDB
"""
from pathlib import Path
import sys
from typing import List
from tqdm import tqdm
from loguru import logger

# Ajout du path
sys.path.append(str(Path(__file__).parent.parent))

from src.config.settings import settings
from src.document_processing.parser import DocumentParser
from src.document_processing.chunker import SemanticChunker
from src.document_processing.embedding_generator import EmbeddingGenerator
from src.storage.vector_store import VectorStore
from src.retrieval.hybrid_search import HybridSearchEngine


class DocumentIngestion:
    """Pipeline d'ingestion de documents"""
    
    def __init__(self):
        """Initialise le pipeline d'ingestion"""
        logger.info("üöÄ Initialisation du pipeline d'ingestion")
        
        # 1. Parser
        self.parser = DocumentParser()
        
        # 2. Chunker
        self.chunker = SemanticChunker(
            chunk_size=settings.CHUNK_SIZE,
            chunk_overlap=settings.CHUNK_OVERLAP,
            min_chunk_size=settings.MIN_CHUNK_SIZE
        )
        
        # 3. Embedding Generator
        self.embedder = EmbeddingGenerator(
            model_name=settings.EMBEDDING_MODEL,
            batch_size=settings.BATCH_SIZE
        )
        
        # 4. Vector Store
        self.vector_store = VectorStore(
            persist_directory=str(settings.CHROMA_PERSIST_DIR),
            collection_name=settings.CHROMA_COLLECTION_NAME
        )
        
        # 5. Hybrid Search (pour BM25)
        self.hybrid_search = HybridSearchEngine(
            vector_store=self.vector_store,
            semantic_weight=settings.SEMANTIC_WEIGHT,
            bm25_weight=settings.BM25_WEIGHT
        )
        
        logger.success("‚úÖ Pipeline initialis√©")
    
    def ingest_document(self, file_path: Path) -> int:
        """
        Ing√®re un document unique
        
        Args:
            file_path: Chemin vers le document
            
        Returns:
            Nombre de chunks cr√©√©s
        """
        logger.info(f"üìÑ Traitement de: {file_path.name}")
        
        try:
            # 1. Parsing
            parsed_doc = self.parser.parse(file_path)
            logger.info(f"  ‚úì Parsing: {len(parsed_doc.content)} caract√®res")
            
            # 2. Chunking
            if parsed_doc.pages:
                # Pour les PDFs avec structure de pages
                chunks = self.chunker.chunk_with_pages(
                    pages_data=parsed_doc.pages,
                    doc_metadata=parsed_doc.metadata
                )
            else:
                # Pour les autres formats
                chunks = self.chunker.chunk_text(
                    text=parsed_doc.content,
                    doc_metadata=parsed_doc.metadata,
                    preserve_structure=True
                )
            
            logger.info(f"  ‚úì Chunking: {len(chunks)} chunks cr√©√©s")
            
            if not chunks:
                logger.warning(f"  ‚ö†Ô∏è Aucun chunk cr√©√© pour {file_path.name}")
                return 0
            
            # 3. G√©n√©ration d'embeddings
            texts = [chunk.text for chunk in chunks]
            embeddings = self.embedder.generate_embeddings_batch(
                texts=texts,
                show_progress=True
            )
            logger.info(f"  ‚úì Embeddings: {len(embeddings)} g√©n√©r√©s")
            
            # 4. Pr√©paration des m√©tadonn√©es
            metadatas = []
            ids = []
            
            for chunk in chunks:
                metadatas.append({
                    'chunk_id': chunk.chunk_id,
                    'filename': chunk.metadata.get('filename'),
                    'filepath': chunk.metadata.get('filepath'),
                    'page_number': chunk.metadata.get('page_number', ''),
                    'section': chunk.metadata.get('section', ''),
                    'char_count': chunk.metadata.get('char_count'),
                    'word_count': chunk.metadata.get('word_count'),
                    'token_count': chunk.token_count,
                    'format': chunk.metadata.get('format'),
                })
                ids.append(chunk.chunk_id)
            
            # 5. Ajout au vector store
            self.vector_store.add_documents(
                texts=texts,
                metadatas=metadatas,
                ids=ids,
                embeddings=embeddings.tolist()
            )
            logger.info(f"  ‚úì Stockage: {len(texts)} chunks ajout√©s √† ChromaDB")
            
            # 6. Indexation BM25
            documents_for_bm25 = [
                {
                    'id': chunk_id,
                    'text': text,
                    'metadata': meta
                }
                for chunk_id, text, meta in zip(ids, texts, metadatas)
            ]
            
            # Note: On devrait accumuler tous les documents puis indexer BM25 √† la fin
            # Pour l'instant, on peut sauter cette √©tape et l'indexer lors du premier usage
            
            logger.success(f"‚úÖ {file_path.name}: {len(chunks)} chunks ing√©r√©s")
            
            return len(chunks)
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du traitement de {file_path.name}: {e}")
            return 0
    
    def ingest_directory(self, directory: Path, recursive: bool = True) -> dict:
        """
        Ing√®re tous les documents d'un r√©pertoire
        
        Args:
            directory: R√©pertoire source
            recursive: Parcours r√©cursif des sous-dossiers
            
        Returns:
            Statistiques d'ingestion
        """
        logger.info(f"üìÅ Ingestion du r√©pertoire: {directory}")
        
        # Collecter les fichiers
        files = []
        for ext in settings.SUPPORTED_FORMATS:
            if recursive:
                files.extend(directory.rglob(f"*{ext}"))
            else:
                files.extend(directory.glob(f"*{ext}"))
        
        logger.info(f"üìä {len(files)} fichiers trouv√©s")
        
        if not files:
            logger.warning("Aucun fichier √† traiter")
            return {'total': 0, 'success': 0, 'failed': 0, 'chunks': 0}
        
        # Ingestion avec barre de progression
        stats = {
            'total': len(files),
            'success': 0,
            'failed': 0,
            'chunks': 0
        }
        
        for file_path in tqdm(files, desc="Ingestion"):
            try:
                num_chunks = self.ingest_document(file_path)
                if num_chunks > 0:
                    stats['success'] += 1
                    stats['chunks'] += num_chunks
                else:
                    stats['failed'] += 1
            except Exception as e:
                logger.error(f"Erreur: {e}")
                stats['failed'] += 1
        
        # Indexation BM25 finale
        logger.info("üîç Indexation BM25...")
        self._index_bm25()
        
        # Rapport final
        logger.info("=" * 60)
        logger.info("üìä RAPPORT D'INGESTION")
        logger.info("=" * 60)
        logger.info(f"Fichiers trait√©s: {stats['total']}")
        logger.info(f"  ‚úÖ Succ√®s: {stats['success']}")
        logger.info(f"  ‚ùå √âchecs: {stats['failed']}")
        logger.info(f"  üì¶ Chunks cr√©√©s: {stats['chunks']}")
        logger.info(f"  üíæ Total en base: {self.vector_store.count()}")
        logger.info("=" * 60)
        
        return stats
    
    def _index_bm25(self):
        """Indexe tous les documents pour BM25"""
        try:
            # R√©cup√©rer tous les documents
            all_docs = self.vector_store.peek(limit=self.vector_store.count())
            
            if not all_docs or not all_docs.get('documents'):
                logger.warning("Aucun document √† indexer pour BM25")
                return
            
            # Pr√©parer les documents pour BM25
            documents = []
            for doc_id, text, metadata in zip(
                all_docs['ids'],
                all_docs['documents'],
                all_docs['metadatas']
            ):
                documents.append({
                    'id': doc_id,
                    'text': text,
                    'metadata': metadata
                })
            
            # Indexation
            self.hybrid_search.index_documents(documents)
            logger.success(f"‚úÖ {len(documents)} documents index√©s pour BM25")
            
        except Exception as e:
            logger.error(f"Erreur indexation BM25: {e}")
    
    def reset_database(self):
        """R√©initialise compl√®tement la base de donn√©es"""
        logger.warning("‚ö†Ô∏è R√âINITIALISATION DE LA BASE")
        confirmation = input("Confirmer la suppression de tous les documents? (oui/non): ")
        
        if confirmation.lower() == 'oui':
            self.vector_store.reset()
            logger.success("‚úÖ Base r√©initialis√©e")
        else:
            logger.info("Op√©ration annul√©e")
    
    def get_stats(self) -> dict:
        """Retourne les statistiques de la base"""
        return {
            'total_documents': self.vector_store.count(),
            'collection_name': settings.CHROMA_COLLECTION_NAME,
            'embedding_model': settings.EMBEDDING_MODEL,
            'embedding_dimension': settings.EMBEDDING_DIMENSION,
            'chunk_size': settings.CHUNK_SIZE,
            'chunk_overlap': settings.CHUNK_OVERLAP
        }


def main():
    """Fonction principale CLI"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Ingestion de documents dans le syst√®me RAG INPT"
    )
    
    parser.add_argument(
        'path',
        type=str,
        help='Chemin vers un fichier ou dossier √† ing√©rer'
    )
    
    parser.add_argument(
        '--recursive',
        '-r',
        action='store_true',
        help='Parcours r√©cursif des sous-dossiers'
    )
    
    parser.add_argument(
        '--reset',
        action='store_true',
        help='R√©initialiser la base avant ingestion'
    )
    
    parser.add_argument(
        '--stats',
        action='store_true',
        help='Afficher les statistiques uniquement'
    )
    
    args = parser.parse_args()
    
    # Configuration du logging
    logger.add(
        "logs/ingestion_{time}.log",
        rotation="10 MB",
        level="INFO"
    )
    
    # Initialisation
    ingestion = DocumentIngestion()
    
    # Stats uniquement
    if args.stats:
        stats = ingestion.get_stats()
        print("\nüìä STATISTIQUES DE LA BASE")
        print("=" * 50)
        for key, value in stats.items():
            print(f"{key}: {value}")
        print("=" * 50)
        return
    
    # Reset si demand√©
    if args.reset:
        ingestion.reset_database()
    
    # V√©rification du path
    path = Path(args.path)
    
    if not path.exists():
        logger.error(f"‚ùå Chemin introuvable: {path}")
        sys.exit(1)
    
    # Ingestion
    if path.is_file():
        logger.info("Mode: Fichier unique")
        num_chunks = ingestion.ingest_document(path)
        logger.info(f"‚úÖ Termin√©: {num_chunks} chunks cr√©√©s")
        
    elif path.is_dir():
        logger.info("Mode: R√©pertoire")
        stats = ingestion.ingest_directory(path, recursive=args.recursive)
        
    else:
        logger.error("‚ùå Type de chemin non support√©")
        sys.exit(1)
    
    # Stats finales
    final_stats = ingestion.get_stats()
    logger.info(f"\nüíæ Total en base: {final_stats['total_documents']} chunks")


if __name__ == "__main__":
    # Exemples d'utilisation:
    # python scripts/ingest_documents.py data/documents
    # python scripts/ingest_documents.py data/documents --recursive
    # python scripts/ingest_documents.py data/documents/cours_iot.pdf
    # python scripts/ingest_documents.py --stats
    # python scripts/ingest_documents.py data/documents --reset
    
    main()
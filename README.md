# ğŸ“ Assistant Ã‰ducatif RAG - INPT Smart ICT

Assistant intelligent basÃ© sur RAG (Retrieval-Augmented Generation) conÃ§u spÃ©cifiquement pour les Ã©tudiants Smart ICT de l'Institut National des Postes et TÃ©lÃ©communications (INPT).

![Version](https://img.shields.io/badge/version-1.0.0-blue)
![Python](https://img.shields.io/badge/python-3.9+-green)
![License](https://img.shields.io/badge/license-MIT-orange)

## ğŸ“‹ Table des MatiÃ¨res

- [FonctionnalitÃ©s](#-fonctionnalitÃ©s)
- [Architecture](#-architecture)
- [Installation](#-installation)
- [Configuration](#-configuration)
- [Utilisation](#-utilisation)
- [Structure du Projet](#-structure-du-projet)
- [DÃ©veloppement](#-dÃ©veloppement)
- [FAQ](#-faq)

## âœ¨ FonctionnalitÃ©s

### ğŸ” Recherche AvancÃ©e
- **Recherche hybride** : Combine recherche sÃ©mantique (embeddings) et recherche par mots-clÃ©s (BM25)
- **Multi-format** : Support de PDF, TXT, MD, DOCX
- **Chunking intelligent** : DÃ©coupage sÃ©mantique prÃ©servant la structure des documents
- **Re-ranking** : AmÃ©lioration de la pertinence des rÃ©sultats

### ğŸ’¬ Chat Intelligent
- **Conversation contextuelle** : Maintien du contexte sur plusieurs Ã©changes
- **Citations prÃ©cises** : RÃ©fÃ©rences aux sources avec numÃ©ros de page
- **Confiance** : Indicateur de confiance pour chaque rÃ©ponse
- **Multilingue** : OptimisÃ© pour le franÃ§ais

### ğŸ“š Gestion Documentaire
- **Ingestion automatique** : Pipeline de traitement de documents
- **MÃ©tadonnÃ©es** : Extraction et indexation des mÃ©tadonnÃ©es
- **Versioning** : Suivi des versions de documents
- **Stockage vectoriel** : ChromaDB pour recherche rapide

### ğŸ¯ PÃ©dagogique
- **Explications progressives** : Adaptation au niveau de l'Ã©tudiant
- **Questions de suivi** : GÃ©nÃ©ration automatique de questions
- **Exercices** : CrÃ©ation d'exercices pratiques
- **Feedback** : Ã‰valuation constructive des rÃ©ponses

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Interface Streamlit                â”‚
â”‚              (Chat + Upload + Analytics)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Response Generator                     â”‚
â”‚    (Orchestration: Recherche + LLM + Post-proc)    â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                                     â”‚
â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Hybrid Search     â”‚          â”‚   Ollama Client   â”‚
â”‚  (Semantic + BM25) â”‚          â”‚   (Llama 3.2)     â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Vector Store     â”‚
â”‚    (ChromaDB)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Document Pipeline â”‚
â”‚ Parser â†’ Chunker   â”‚
â”‚    â†’ Embedder      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Composants Principaux

1. **Document Processing** : Parsing, chunking sÃ©mantique, gÃ©nÃ©ration d'embeddings
2. **Storage Layer** : ChromaDB (vecteurs) + SQLite (mÃ©tadonnÃ©es)
3. **Retrieval Engine** : Recherche hybride avec re-ranking
4. **LLM Integration** : Ollama pour gÃ©nÃ©ration locale
5. **Conversation Manager** : Gestion de l'historique et du contexte
6. **Web Interface** : Streamlit pour l'UI

## ğŸš€ Installation

### PrÃ©requis

- Python 3.9+
- Ollama installÃ© et en cours d'exÃ©cution
- 8GB RAM minimum (16GB recommandÃ©)
- GPU optionnel mais recommandÃ©

### Installation Rapide

```bash
# 1. Cloner le repository
git clone https://github.com/votre-repo/inpt-rag-assistant.git
cd inpt-rag-assistant

# 2. CrÃ©er un environnement virtuel
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

# 3. Installer les dÃ©pendances
pip install -r requirements.txt

# 4. Installer spaCy franÃ§ais
python -m spacy download fr_core_news_md

# 5. Installer Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# 6. TÃ©lÃ©charger le modÃ¨le LLM
ollama pull llama3.2:3b

# 7. Copier et configurer .env
cp .env.example .env
# Ã‰diter .env selon vos besoins

# 8. Initialiser les dossiers
python -c "from src.config.settings import setup_directories; setup_directories()"
```

## âš™ï¸ Configuration

### Variables d'Environnement

Ã‰ditez le fichier `.env` :

```bash
# ModÃ¨le LLM (choisir selon vos ressources)
OLLAMA_MODEL="llama3.2:3b"  # LÃ©ger, rapide (3GB RAM)
# OLLAMA_MODEL="mistral:7b"  # Meilleur qualitÃ© (8GB RAM)

# Embeddings
EMBEDDING_MODEL="paraphrase-multilingual-MiniLM-L12-v2"

# Recherche
SEMANTIC_WEIGHT=0.7  # Poids recherche sÃ©mantique
BM25_WEIGHT=0.3      # Poids recherche mots-clÃ©s
```

### ModÃ¨les RecommandÃ©s

**LLM (Ollama):**
- `llama3.2:3b` - LÃ©ger et rapide (3GB)
- `llama3.2:7b` - Ã‰quilibrÃ© (7GB)
- `mistral:7b` - Excellente qualitÃ© (8GB)

**Embeddings:**
- `paraphrase-multilingual-MiniLM-L12-v2` - Rapide, 384 dim
- `paraphrase-multilingual-mpnet-base-v2` - Meilleure qualitÃ©, 768 dim

## ğŸ“– Utilisation

### 1. Ingestion de Documents

```bash
# Lancer Ollama en arriÃ¨re-plan
ollama serve &

# IngÃ©rer un dossier complet
python scripts/ingest_documents.py data/documents --recursive

# IngÃ©rer un fichier unique
python scripts/ingest_documents.py data/documents/cours_iot.pdf

# RÃ©initialiser et rÃ©ingÃ©rer
python scripts/ingest_documents.py data/documents --reset --recursive

# Voir les statistiques
python scripts/ingest_documents.py --stats
```

### 2. Lancer l'Application

```bash
# Lancer Streamlit
streamlit run app/streamlit_app.py

# Ou avec configuration personnalisÃ©e
streamlit run app/streamlit_app.py --server.port 8501
```

L'application sera accessible sur `http://localhost:8501`

### 3. Utilisation via Python

```python
from src.config.settings import settings
from src.storage.vector_store import VectorStore
from src.retrieval.hybrid_search import HybridSearchEngine
from src.llm.ollama_client import OllamaClient
from src.llm.response_generator import ResponseGenerator
from src.llm.prompt_templates import PromptBuilder

# Initialisation
vector_store = VectorStore(
    persist_directory=str(settings.CHROMA_PERSIST_DIR),
    collection_name=settings.CHROMA_COLLECTION_NAME
)

hybrid_search = HybridSearchEngine(vector_store=vector_store)
ollama = OllamaClient(model=settings.OLLAMA_MODEL)
prompt_builder = PromptBuilder()

response_gen = ResponseGenerator(
    hybrid_search=hybrid_search,
    ollama_client=ollama,
    prompt_builder=prompt_builder
)

# Poser une question
response = response_gen.generate_response(
    question="Qu'est-ce que l'IoT ?",
    temperature=0.7
)

print(response.answer)
print(f"Confiance: {response.confidence:.2%}")
print(f"Sources: {len(response.sources)}")
```

## ğŸ“ Structure du Projet

```
inpt-rag-assistant/
â”œâ”€â”€ src/                          # Code source
â”‚   â”œâ”€â”€ config/                   # Configuration
â”‚   â”‚   â””â”€â”€ settings.py
â”‚   â”œâ”€â”€ document_processing/      # Traitement documents
â”‚   â”‚   â”œâ”€â”€ parser.py
â”‚   â”‚   â”œâ”€â”€ chunker.py
â”‚   â”‚   â”œâ”€â”€ metadata_extractor.py
â”‚   â”‚   â””â”€â”€ embedding_generator.py
â”‚   â”œâ”€â”€ storage/                  # Couche stockage
â”‚   â”‚   â”œâ”€â”€ vector_store.py
â”‚   â”‚   â”œâ”€â”€ metadata_store.py
â”‚   â”‚   â””â”€â”€ models.py
â”‚   â”œâ”€â”€ retrieval/                # Moteur de recherche
â”‚   â”‚   â”œâ”€â”€ hybrid_search.py
â”‚   â”‚   â”œâ”€â”€ bm25_retriever.py
â”‚   â”‚   â””â”€â”€ reranker.py
â”‚   â”œâ”€â”€ llm/                      # IntÃ©gration LLM
â”‚   â”‚   â”œâ”€â”€ ollama_client.py
â”‚   â”‚   â”œâ”€â”€ prompt_templates.py
â”‚   â”‚   â””â”€â”€ response_generator.py
â”‚   â”œâ”€â”€ conversation/             # Gestion conversations
â”‚   â”‚   â”œâ”€â”€ manager.py
â”‚   â”‚   â””â”€â”€ context_window.py
â”‚   â”œâ”€â”€ utils/                    # Utilitaires
â”‚   â”‚   â”œâ”€â”€ query_enhancement.py
â”‚   â”‚   â”œâ”€â”€ text_processing.py
â”‚   â”‚   â””â”€â”€ logger.py
â”‚   â””â”€â”€ analytics/                # Analytics
â”‚       â”œâ”€â”€ tracker.py
â”‚       â””â”€â”€ metrics.py
â”œâ”€â”€ app/                          # Application Streamlit
â”‚   â”œâ”€â”€ streamlit_app.py
â”‚   â””â”€â”€ pages/
â”‚       â”œâ”€â”€ chat.py
â”‚       â”œâ”€â”€ upload.py
â”‚       â””â”€â”€ analytics.py
â”œâ”€â”€ data/                         # DonnÃ©es
â”‚   â”œâ”€â”€ documents/                # Documents sources
â”‚   â””â”€â”€ processed/                # Documents traitÃ©s
â”œâ”€â”€ database/                     # Bases de donnÃ©es
â”‚   â”œâ”€â”€ chroma_db/               # ChromaDB
â”‚   â””â”€â”€ metadata.db              # SQLite
â”œâ”€â”€ scripts/                      # Scripts utilitaires
â”‚   â”œâ”€â”€ ingest_documents.py
â”‚   â”œâ”€â”€ setup_database.py
â”‚   â””â”€â”€ benchmark.py
â”œâ”€â”€ tests/                        # Tests
â”œâ”€â”€ logs/                         # Logs
â”œâ”€â”€ requirements.txt              # DÃ©pendances
â”œâ”€â”€ .env.example                  # Config exemple
â””â”€â”€ README.md                     # Documentation
```

## ğŸ§ª Tests

```bash
# Lancer tous les tests
pytest tests/

# Tests avec couverture
pytest tests/ --cov=src --cov-report=html

# Test d'un composant spÃ©cifique
pytest tests/test_document_processing.py -v
```

## ğŸ› ï¸ DÃ©veloppement

### Ajout d'un Nouveau Format de Document

1. Modifier `src/document_processing/parser.py`
2. Ajouter le parser spÃ©cifique
3. Mettre Ã  jour `SUPPORTED_FORMATS` dans `settings.py`

### Changement de ModÃ¨le LLM

```bash
# TÃ©lÃ©charger un nouveau modÃ¨le
ollama pull mistral:7b

# Mettre Ã  jour .env
OLLAMA_MODEL="mistral:7b"

# RedÃ©marrer l'application
```

### Personnalisation des Prompts

Ã‰diter `src/llm/prompt_templates.py` pour modifier les templates de prompts selon vos besoins.

## ğŸ“Š Performances

### Benchmarks (Machine de rÃ©fÃ©rence: i7, 16GB RAM)

- **Ingestion**: ~50 pages PDF/minute
- **Recherche**: ~100ms par requÃªte
- **GÃ©nÃ©ration**: ~2-5 secondes (selon modÃ¨le)
- **Embedding**: ~1000 chunks/minute

### Optimisations

```python
# Augmenter le batch size pour l'ingestion
BATCH_SIZE=64

# RÃ©duire le nombre de rÃ©sultats de recherche
TOP_K_RETRIEVAL=5

# Utiliser un modÃ¨le plus lÃ©ger
OLLAMA_MODEL="llama3.2:3b"
```

## â“ FAQ

**Q: Ollama ne se connecte pas**  
A: VÃ©rifiez que le service est lancÃ©: `ollama serve`

**Q: Erreur "Out of memory"**  
A: Utilisez un modÃ¨le plus lÃ©ger ou rÃ©duisez `BATCH_SIZE`

**Q: Les rÃ©ponses sont lentes**  
A: Utilisez un GPU ou un modÃ¨le plus petit (llama3.2:3b)

**Q: Comment ajouter des documents en cours d'exÃ©cution?**  
A: Utilisez le script d'ingestion pendant que l'app tourne, ou uploadez via l'interface (Ã  implÃ©menter)

**Q: Les embeddings sont lents**  
A: Utilisez un GPU ou rÃ©duisez `BATCH_SIZE`

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! Voir [CONTRIBUTING.md](CONTRIBUTING.md)

## ğŸ“ License

Ce projet est sous licence MIT. Voir [LICENSE](LICENSE)

## ğŸ‘¥ Auteurs

- DÃ©veloppÃ© pour l'INPT Smart ICT
- PropulsÃ© par Ollama, ChromaDB, Streamlit

## ğŸ“§ Support

Pour toute question ou problÃ¨me:
- Ouvrir une issue sur GitHub
- Contacter: support@inpt.ac.ma

---

**Version**: 1.0.0  
**DerniÃ¨re mise Ã  jour**: DÃ©cembre 2024
# üéì Assistant √âducatif RAG - INPT Smart ICT

**Projet Acad√©mique - Syst√®me de Recherche et G√©n√©ration Augment√©e par R√©cup√©ration**

![Python](https://img.shields.io/badge/python-3.11+-blue)
![Streamlit](https://img.shields.io/badge/streamlit-1.52.1-red)
![ChromaDB](https://img.shields.io/badge/chromadb-1.3.5-green)
![License](https://img.shields.io/badge/license-MIT-orange)

---

## üìã Pr√©sentation du Projet

Ce projet impl√©mente un **syst√®me RAG (Retrieval-Augmented Generation)** intelligent con√ßu pour assister les √©tudiants Smart ICT de l'Institut National des Postes et T√©l√©communications (INPT). Le syst√®me combine des techniques avanc√©es de traitement du langage naturel, de recherche vectorielle et de g√©n√©ration de texte pour cr√©er un assistant √©ducatif capable de r√©pondre aux questions des √©tudiants en se basant sur leurs documents de cours.

### üéØ Objectifs P√©dagogiques

1. **Apprentissage Personnalis√©** : Fournir des r√©ponses contextualis√©es bas√©es sur le contenu sp√©cifique des cours
2. **Accessibilit√©** : Permettre aux √©tudiants d'interroger leurs documents en langage naturel
3. **Tra√ßabilit√©** : Citer pr√©cis√©ment les sources utilis√©es pour chaque r√©ponse
4. **Multimodalit√©** : Supporter diff√©rents formats de documents (PDF, TXT, MD, DOCX)

---

## üèóÔ∏è Architecture Technique

### Vue d'Ensemble du Syst√®me

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                Interface Streamlit                  ‚îÇ
‚îÇ         (Chat + Upload + Analytics)                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Response Generator                     ‚îÇ
‚îÇ    (Orchestration: Recherche + LLM + Post-proc)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ                                     ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Hybrid Search     ‚îÇ          ‚îÇ   Ollama Client   ‚îÇ
‚îÇ  (Semantic + BM25) ‚îÇ          ‚îÇ   (Llama 3.2)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Vector Store     ‚îÇ
‚îÇ    (ChromaDB)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Document Pipeline ‚îÇ
‚îÇ Parser ‚Üí Chunker   ‚îÇ
‚îÇ    ‚Üí Embedder      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### üîß Composants Principaux

#### 1. **Pipeline de Traitement de Documents** (`src/document_processing/`)
- **Parser** : Extraction de texte multi-format (PDF, TXT, MD, DOCX)
- **Chunker S√©mantique** : D√©coupage intelligent pr√©servant la coh√©rence
- **G√©n√©rateur d'Embeddings** : Vectorisation avec mod√®les multilingues
- **Extracteur de M√©tadonn√©es** : Enrichissement contextuel

#### 2. **Moteur de Recherche Hybride** (`src/retrieval/`)
- **Recherche S√©mantique** : Similarit√© vectorielle avec embeddings
- **Recherche BM25** : Correspondance par mots-cl√©s (TF-IDF am√©lior√©)
- **Fusion de Scores** : Combinaison pond√©r√©e des deux approches
- **Re-ranking** : Am√©lioration de la pertinence des r√©sultats

#### 3. **Couche de Stockage** (`src/storage/`)
- **ChromaDB** : Base de donn√©es vectorielle pour embeddings
- **SQLite** : M√©tadonn√©es et historique des conversations
- **Mod√®les de Donn√©es** : Structures optimis√©es pour la recherche

#### 4. **Int√©gration LLM** (`src/llm/`)
- **Client Ollama** : Interface avec mod√®les locaux (Llama 3.2)
- **Templates de Prompts** : Prompts optimis√©s pour l'√©ducation
- **G√©n√©rateur de R√©ponses** : Orchestration RAG compl√®te

#### 5. **Interface Utilisateur** (`app/`)
- **Streamlit** : Interface web moderne et intuitive
- **Chat Interface** : Conversation naturelle avec l'assistant
- **Upload de Documents** : Ingestion en temps r√©el
- **Analytics** : M√©triques et statistiques d'utilisation

---

## üöÄ Installation et Configuration

### Pr√©requis Syst√®me

- **Python 3.11+** (test√© avec 3.11.14)
- **Ollama** ([https://ollama.ai](https://ollama.ai))
- **8GB RAM minimum** (16GB recommand√©)
- **10GB espace disque** pour mod√®les et donn√©es

### Installation Rapide

```bash
# 1. Cloner le projet
git clone https://github.com/SouhailBourhim/ICT_BOT_V2.git
cd ICT_BOT_V2

# 2. Cr√©er l'environnement virtuel
python3.11 -m venv venv
source venv/bin/activate  # Linux/Mac
# ou venv\Scripts\activate  # Windows

# 3. Installer les d√©pendances
pip install -r requirements.txt

# 4. Configurer Ollama
ollama serve &
ollama pull llama3.2:3b

# 5. Initialiser la base de donn√©es
python scripts/setup_database.py

# 6. Configurer l'environnement
cp .env.example .env
# √âditer .env selon vos besoins
```

### Configuration Avanc√©e

Le fichier `.env` permet de personnaliser le comportement du syst√®me :

```bash
# Mod√®le LLM
OLLAMA_MODEL="llama3.2:3b"
LLM_TEMPERATURE=0.1
LLM_MAX_TOKENS=500

# Embeddings
EMBEDDING_MODEL="paraphrase-multilingual-MiniLM-L12-v2"
BATCH_SIZE=32

# Recherche Hybride
SEMANTIC_WEIGHT=0.7  # 70% recherche s√©mantique
BM25_WEIGHT=0.3      # 30% recherche par mots-cl√©s
TOP_K_RETRIEVAL=10

# Chunking
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
```

---

## üìö Utilisation du Syst√®me

### 1. Ingestion de Documents

```bash
# Ajouter des documents au dossier
cp ~/cours/*.pdf data/documents/

# Ing√©rer tous les documents
python scripts/ingest_documents.py data/documents --recursive

# V√©rifier les statistiques
python scripts/ingest_documents.py --stats
```

### 2. Lancement de l'Application

```bash
# D√©marrer Ollama (si pas d√©j√† fait)
ollama serve &

# Lancer l'interface Streamlit
streamlit run app/streamlit_app.py
```

L'application sera accessible sur `http://localhost:8501`

### 3. Utilisation via Interface Web

1. **Page Chat** : Poser des questions en langage naturel
2. **Upload Documents** : Ajouter de nouveaux documents
3. **Analytics** : Consulter les m√©triques du syst√®me

### 4. Exemples de Questions

```
- "Qu'est-ce que le clustering en machine learning ?"
- "Explique-moi l'algorithme K-means"
- "Quelles sont les diff√©rences entre apprentissage supervis√© et non-supervis√© ?"
- "Comment fonctionne la r√©gression lin√©aire ?"
```

---

## üî¨ Aspects Techniques Avanc√©s

### Algorithmes de Recherche

#### Recherche S√©mantique
- **Mod√®le d'Embeddings** : `paraphrase-multilingual-MiniLM-L12-v2`
- **Dimension des Vecteurs** : 384
- **M√©trique de Similarit√©** : Cosinus
- **Optimisation** : Index HNSW pour recherche rapide

#### Recherche BM25
- **Tokenisation** : NLTK avec support fran√ßais
- **Param√®tres** : k1=1.2, b=0.75 (optimis√©s pour textes acad√©miques)
- **Pr√©processing** : Lemmatisation et suppression des mots vides

#### Fusion de Scores
```python
score_final = (semantic_weight * score_semantic) + (bm25_weight * score_bm25)
```

### Pipeline de Traitement

#### Chunking S√©mantique
- **Strat√©gie** : Pr√©servation des paragraphes et sections
- **Taille** : 1000 caract√®res avec overlap de 200
- **M√©tadonn√©es** : Page, section, type de contenu

#### G√©n√©ration de R√©ponses
- **Contexte** : Top-K chunks les plus pertinents
- **Prompt Engineering** : Templates optimis√©s pour l'√©ducation
- **Post-processing** : Validation et formatage des r√©ponses

---

## ÔøΩ Structure du Projet

```
inpt-rag-assistant/
‚îú‚îÄ‚îÄ src/                          # Code source principal
‚îÇ   ‚îú‚îÄ‚îÄ config/                   # Configuration syst√®me
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ settings.py          # Param√®tres globaux
‚îÇ   ‚îú‚îÄ‚îÄ document_processing/      # Pipeline de traitement
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ parser.py            # Extraction multi-format
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chunker.py           # D√©coupage s√©mantique
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embedding_generator.py # Vectorisation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ metadata_extractor.py # Enrichissement
‚îÇ   ‚îú‚îÄ‚îÄ storage/                  # Couche de persistance
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vector_store.py      # Interface ChromaDB
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metadata_store.py    # Gestion m√©tadonn√©es
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ models.py            # Mod√®les de donn√©es
‚îÇ   ‚îú‚îÄ‚îÄ retrieval/                # Moteur de recherche
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hybrid_search.py     # Recherche hybride
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ semantic_retriever.py # Recherche vectorielle
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ bm25_retriever.py    # Recherche BM25
‚îÇ   ‚îú‚îÄ‚îÄ llm/                      # Int√©gration LLM
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ollama_client.py     # Client Ollama
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prompt_templates.py  # Templates de prompts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ response_generator.py # G√©n√©ration RAG
‚îÇ   ‚îú‚îÄ‚îÄ conversation/             # Gestion conversations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ manager.py           # Historique et contexte
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ context_window.py    # Fen√™tre contextuelle
‚îÇ   ‚îî‚îÄ‚îÄ utils/                    # Utilitaires
‚îÇ       ‚îú‚îÄ‚îÄ text_processing.py   # Traitement de texte
‚îÇ       ‚îî‚îÄ‚îÄ logger.py            # Syst√®me de logs
‚îú‚îÄ‚îÄ app/                          # Interface Streamlit
‚îÇ   ‚îú‚îÄ‚îÄ streamlit_app.py         # Application principale
‚îÇ   ‚îú‚îÄ‚îÄ components/              # Composants UI
‚îÇ   ‚îî‚îÄ‚îÄ pages/                   # Pages de l'interface
‚îú‚îÄ‚îÄ scripts/                      # Scripts utilitaires
‚îÇ   ‚îú‚îÄ‚îÄ ingest_documents.py      # Ingestion de documents
‚îÇ   ‚îú‚îÄ‚îÄ setup_database.py        # Initialisation DB
‚îÇ   ‚îî‚îÄ‚îÄ benchmark.py             # Tests de performance
‚îú‚îÄ‚îÄ tests/                        # Tests unitaires
‚îú‚îÄ‚îÄ docker/                       # Configuration Docker
‚îú‚îÄ‚îÄ data/                         # Donn√©es
‚îÇ   ‚îú‚îÄ‚îÄ documents/               # Documents sources
‚îÇ   ‚îî‚îÄ‚îÄ conversations/           # Historique des chats
‚îú‚îÄ‚îÄ database/                     # Bases de donn√©es
‚îÇ   ‚îî‚îÄ‚îÄ chroma_db/              # ChromaDB
‚îú‚îÄ‚îÄ requirements.txt              # D√©pendances Python
‚îú‚îÄ‚îÄ .env.example                 # Configuration exemple
‚îî‚îÄ‚îÄ README.md                    # Documentation
```

---

## üß™ Tests et √âvaluation

### Tests Unitaires

```bash
# Lancer tous les tests
pytest tests/ -v

# Tests avec couverture
pytest tests/ --cov=src --cov-report=html

# Test d'un composant sp√©cifique
pytest tests/test_document_processing.py -v
```

### √âvaluation du Syst√®me

Le syst√®me inclut des m√©triques d'√©valuation automatiques :

- **Pr√©cision de Recherche** : Pertinence des documents r√©cup√©r√©s
- **Qualit√© des R√©ponses** : Coh√©rence et exactitude
- **Performance** : Temps de r√©ponse et utilisation m√©moire
- **Couverture** : Pourcentage de questions avec r√©ponses satisfaisantes

### Benchmarks de Performance

Sur une machine de r√©f√©rence (i7, 16GB RAM) :
- **Ingestion** : ~50 pages PDF/minute
- **Recherche** : ~100ms par requ√™te
- **G√©n√©ration** : ~2-5 secondes selon le mod√®le
- **Embedding** : ~1000 chunks/minute

---

## üê≥ D√©ploiement Docker

Pour un d√©ploiement simplifi√© :

```bash
# D√©ploiement complet
cd docker
docker-compose up -d

# Acc√®s √† l'application
open http://localhost:8501
```

Le syst√®me Docker inclut :
- **Application principale** avec Streamlit
- **Service Ollama** pour les LLMs
- **Volumes persistants** pour donn√©es et mod√®les
- **Configuration r√©seau** optimis√©e

---

## üìà M√©triques et Analytics

### Tableau de Bord Analytics

L'interface inclut un tableau de bord complet avec :

- **Statistiques de Documents** : Nombre, taille, formats
- **M√©triques de Recherche** : Requ√™tes, temps de r√©ponse, pertinence
- **Utilisation LLM** : Tokens g√©n√©r√©s, mod√®les utilis√©s
- **Performance Syst√®me** : M√©moire, CPU, stockage

### Logs et Monitoring

```bash
# Consulter les logs
tail -f logs/application.log

# Statistiques de la base vectorielle
python scripts/ingest_documents.py --stats

# M√©triques de performance
python scripts/benchmark.py
```

---

## üîß Personnalisation et Extension

### Ajout de Nouveaux Formats

Pour supporter un nouveau format de document :

1. √âtendre `src/document_processing/parser.py`
2. Ajouter le parser sp√©cifique
3. Mettre √† jour `SUPPORTED_FORMATS` dans `settings.py`

### Modification des Prompts

Les templates de prompts sont dans `src/llm/prompt_templates.py` :

```python
RAG_QA = PromptTemplate(
    system="Tu es un assistant √©ducatif...",
    user="Contexte: {context}\nQuestion: {question}\nR√©ponse:"
)
```

### Int√©gration de Nouveaux Mod√®les

Pour utiliser un autre mod√®le LLM :

```bash
# T√©l√©charger le mod√®le
ollama pull mistral:7b

# Modifier la configuration
echo "OLLAMA_MODEL=mistral:7b" >> .env
```

---

## üéì Aspects P√©dagogiques

### Fonctionnalit√©s √âducatives

1. **R√©ponses Gradu√©es** : Adaptation au niveau de compr√©hension
2. **Citations Pr√©cises** : R√©f√©rences aux sources avec num√©ros de page
3. **Questions de Suivi** : G√©n√©ration automatique pour approfondir
4. **Explications √âtape par √âtape** : D√©composition des concepts complexes

### Optimisations pour l'Apprentissage

- **Prompts P√©dagogiques** : Encouragent la r√©flexion critique
- **Contexte Pr√©serv√©** : Maintien de la coh√©rence conversationnelle
- **Feedback Constructif** : Suggestions d'am√©lioration
- **Multilingue** : Support fran√ßais optimis√©

---

## üîí S√©curit√© et Confidentialit√©

### Protection des Donn√©es

- **Traitement Local** : Aucune donn√©e envoy√©e vers des services externes
- **Chiffrement** : Base de donn√©es et communications s√©curis√©es
- **Isolation** : Environnement containeris√© avec Docker
- **Logs Anonymis√©s** : Pas de stockage d'informations personnelles

### Bonnes Pratiques

- Variables d'environnement pour configuration sensible
- Validation des entr√©es utilisateur
- Gestion s√©curis√©e des fichiers upload√©s
- Limitation des ressources syst√®me

---

## üìö Documentation Technique

### Guides Disponibles

- **QUICKSTART.md** : Guide de d√©marrage rapide
- **DOCKER_GUIDE.md** : D√©ploiement avec Docker
- **EVALUATION_REPORT.md** : Rapport d'√©valuation d√©taill√©
- **MATH_FORMULAS_GUIDE.md** : Support des formules math√©matiques

### API et Int√©gration

Le syst√®me expose des interfaces Python pour int√©gration :

```python
from src.llm.response_generator import ResponseGenerator
from src.retrieval.hybrid_search import HybridSearchEngine

# Initialisation
response_gen = ResponseGenerator(...)

# G√©n√©ration de r√©ponse
response = response_gen.generate_response(
    question="Qu'est-ce que l'IoT ?",
    conversation_id="user_123"
)
```

---

## ü§ù Contribution et D√©veloppement

### Standards de Code

- **PEP 8** : Style de code Python
- **Type Hints** : Annotations de types compl√®tes
- **Docstrings** : Documentation des fonctions
- **Tests** : Couverture minimale de 80%

### Workflow de D√©veloppement

1. Fork du repository
2. Cr√©ation d'une branche feature
3. D√©veloppement avec tests
4. Pull request avec review
5. Int√©gration apr√®s validation

---

## üìû Support et Contact

### Ressources d'Aide

- **Issues GitHub** : Signalement de bugs et demandes de fonctionnalit√©s
- **Documentation** : Guides complets dans le repository
- **Logs** : Diagnostic automatique des probl√®mes
- **Community** : Forum de discussion pour utilisateurs

### Informations de Contact

- **Repository** : https://github.com/SouhailBourhim/ICT_BOT_V2
- **Auteur** : √âtudiant Smart ICT - INPT
- **Encadrement** : Professeurs du d√©partement Smart ICT

---

## üìù Licence et Cr√©dits

### Licence

Ce projet est distribu√© sous licence MIT, permettant :
- Utilisation libre pour l'√©ducation et la recherche
- Modification et redistribution
- Usage commercial avec attribution

### Technologies Utilis√©es

- **Python 3.11** : Langage principal
- **Streamlit** : Interface utilisateur web
- **ChromaDB** : Base de donn√©es vectorielle
- **Ollama** : Orchestration de mod√®les LLM
- **Sentence Transformers** : G√©n√©ration d'embeddings
- **NLTK** : Traitement du langage naturel
- **Docker** : Containerisation et d√©ploiement

### Remerciements

- **INPT** : Institut National des Postes et T√©l√©communications
- **D√©partement Smart ICT** : Encadrement p√©dagogique
- **Communaut√© Open Source** : Outils et biblioth√®ques utilis√©s

---

## üéØ Conclusion

Ce projet d√©montre l'impl√©mentation pratique d'un syst√®me RAG complet, int√©grant les derni√®res avanc√©es en intelligence artificielle pour cr√©er un assistant √©ducatif performant. Il illustre la ma√Ætrise de technologies modernes (LLMs, bases de donn√©es vectorielles, interfaces web) tout en r√©pondant √† un besoin p√©dagogique r√©el.

Le syst√®me est con√ßu pour √™tre :
- **Extensible** : Architecture modulaire permettant l'ajout de fonctionnalit√©s
- **Performant** : Optimisations pour temps de r√©ponse et utilisation m√©moire
- **Robuste** : Gestion d'erreurs et tests automatis√©s
- **Utilisable** : Interface intuitive et documentation compl√®te

**Version** : 1.0.0  
**Date** : D√©cembre 2024  
**Statut** : Production Ready ‚úÖ

---

*D√©velopp√© avec ‚ù§Ô∏è pour l'excellence acad√©mique √† l'INPT Smart ICT*
"""
Client Ollama pour int√©gration LLM locale
Support des mod√®les Llama, Mistral, etc.
"""
from typing import List, Dict, Optional, Generator
import requests
import json
from loguru import logger
import time


class OllamaClient:
    """
    Client pour interagir avec Ollama API
    G√®re la g√©n√©ration de r√©ponses, le streaming, etc.
    """
    
    def __init__(
        self,
        base_url: str = "http://localhost:11434",
        model: str = "llama3.2:3b",
        timeout: int = 120
    ):
        """
        Initialise le client Ollama
        
        Args:
            base_url: URL de l'API Ollama
            model: Nom du mod√®le √† utiliser
            timeout: Timeout des requ√™tes (secondes)
        """
        self.base_url = base_url.rstrip('/')
        self.model = model
        self.timeout = timeout
        
        logger.info(f"Client Ollama initialis√©: {model} @ {base_url}")
        
        # V√©rification de la connexion
        self._check_connection()
    
    def _check_connection(self) -> bool:
        """V√©rifie que Ollama est accessible"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get('models', [])
                logger.success(f"‚úÖ Ollama connect√© ({len(models)} mod√®les disponibles)")
                return True
            else:
                logger.warning(f"‚ö†Ô∏è Ollama r√©pond mais avec code {response.status_code}")
                return False
        except Exception as e:
            logger.error(f"‚ùå Impossible de se connecter √† Ollama: {e}")
            logger.info("Assurez-vous qu'Ollama est lanc√©: ollama serve")
            return False
    
    def generate(
        self,
        prompt: str,
        system: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 2000,
        stop: Optional[List[str]] = None,
        stream: bool = False
    ) -> str:
        """
        G√©n√®re une r√©ponse avec Ollama
        
        Args:
            prompt: Prompt utilisateur
            system: Prompt syst√®me (instructions)
            temperature: Cr√©ativit√© [0-1]
            max_tokens: Nombre max de tokens
            stop: S√©quences d'arr√™t
            stream: Mode streaming
            
        Returns:
            Texte g√©n√©r√©
        """
        endpoint = f"{self.base_url}/api/generate"
        
        payload = {
            "model": self.model,
            "prompt": prompt,
            "stream": stream,
            "options": {
                "temperature": temperature,
                "num_predict": max_tokens,
            }
        }
        
        if system:
            payload["system"] = system
        
        if stop:
            payload["options"]["stop"] = stop
        
        try:
            if stream:
                return self._generate_stream(endpoint, payload)
            else:
                return self._generate_complete(endpoint, payload)
                
        except Exception as e:
            logger.error(f"Erreur lors de la g√©n√©ration: {e}")
            raise
    
    def _generate_complete(self, endpoint: str, payload: Dict) -> str:
        """G√©n√©ration compl√®te (non-streaming)"""
        start_time = time.time()
        
        response = requests.post(
            endpoint,
            json=payload,
            timeout=self.timeout
        )
        
        if response.status_code != 200:
            raise Exception(f"Ollama API error: {response.status_code} - {response.text}")
        
        result = response.json()
        generated_text = result.get('response', '')
        
        # M√©triques
        elapsed = time.time() - start_time
        total_duration = result.get('total_duration', 0) / 1e9  # nanosecondes -> secondes
        
        logger.info(f"‚úÖ G√©n√©ration compl√®te en {elapsed:.2f}s (model: {total_duration:.2f}s)")
        
        return generated_text
    
    def _generate_stream(self, endpoint: str, payload: Dict) -> Generator[str, None, None]:
        """G√©n√©ration en mode streaming"""
        response = requests.post(
            endpoint,
            json=payload,
            stream=True,
            timeout=self.timeout
        )
        
        if response.status_code != 200:
            raise Exception(f"Ollama API error: {response.status_code}")
        
        for line in response.iter_lines():
            if line:
                chunk = json.loads(line)
                if 'response' in chunk:
                    yield chunk['response']
                
                if chunk.get('done', False):
                    break
    
    def chat(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 2000,
        stream: bool = False
    ) -> str:
        """
        Mode chat avec historique de conversation
        
        Args:
            messages: Liste de messages [{"role": "user/assistant", "content": "..."}]
            temperature: Cr√©ativit√©
            max_tokens: Tokens max
            stream: Mode streaming
            
        Returns:
            R√©ponse du mod√®le
        """
        endpoint = f"{self.base_url}/api/chat"
        
        payload = {
            "model": self.model,
            "messages": messages,
            "stream": stream,
            "options": {
                "temperature": temperature,
                "num_predict": max_tokens,
            }
        }
        
        try:
            if stream:
                return self._chat_stream(endpoint, payload)
            else:
                return self._chat_complete(endpoint, payload)
                
        except Exception as e:
            logger.error(f"Erreur lors du chat: {e}")
            raise
    
    def _chat_complete(self, endpoint: str, payload: Dict) -> str:
        """Chat mode complet"""
        response = requests.post(
            endpoint,
            json=payload,
            timeout=self.timeout
        )
        
        if response.status_code != 200:
            raise Exception(f"Ollama API error: {response.status_code}")
        
        result = response.json()
        message = result.get('message', {})
        
        return message.get('content', '')
    
    def _chat_stream(self, endpoint: str, payload: Dict) -> Generator[str, None, None]:
        """Chat mode streaming"""
        response = requests.post(
            endpoint,
            json=payload,
            stream=True,
            timeout=self.timeout
        )
        
        if response.status_code != 200:
            raise Exception(f"Ollama API error: {response.status_code}")
        
        for line in response.iter_lines():
            if line:
                chunk = json.loads(line)
                message = chunk.get('message', {})
                if 'content' in message:
                    yield message['content']
                
                if chunk.get('done', False):
                    break
    
    def create_embeddings(self, text: str) -> List[float]:
        """
        G√©n√®re des embeddings avec Ollama
        (N√©cessite un mod√®le compatible comme nomic-embed-text)
        """
        endpoint = f"{self.base_url}/api/embeddings"
        
        payload = {
            "model": self.model,
            "prompt": text
        }
        
        try:
            response = requests.post(endpoint, json=payload, timeout=self.timeout)
            
            if response.status_code != 200:
                raise Exception(f"Ollama API error: {response.status_code}")
            
            result = response.json()
            return result.get('embedding', [])
            
        except Exception as e:
            logger.error(f"Erreur lors de la g√©n√©ration d'embeddings: {e}")
            raise
    
    def list_models(self) -> List[Dict]:
        """Liste tous les mod√®les disponibles"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=10)
            
            if response.status_code == 200:
                return response.json().get('models', [])
            else:
                return []
                
        except Exception as e:
            logger.error(f"Erreur lors de la r√©cup√©ration des mod√®les: {e}")
            return []
    
    def pull_model(self, model_name: str) -> bool:
        """
        T√©l√©charge un mod√®le Ollama
        
        Args:
            model_name: Nom du mod√®le (ex: "llama3.2:3b")
            
        Returns:
            True si succ√®s
        """
        endpoint = f"{self.base_url}/api/pull"
        
        payload = {"name": model_name}
        
        logger.info(f"T√©l√©chargement du mod√®le {model_name}...")
        
        try:
            response = requests.post(
                endpoint,
                json=payload,
                stream=True,
                timeout=600  # 10 minutes pour le t√©l√©chargement
            )
            
            for line in response.iter_lines():
                if line:
                    status = json.loads(line)
                    if 'status' in status:
                        logger.info(status['status'])
            
            logger.success(f"‚úÖ Mod√®le {model_name} t√©l√©charg√©")
            return True
            
        except Exception as e:
            logger.error(f"Erreur lors du t√©l√©chargement: {e}")
            return False
    
    def delete_model(self, model_name: str) -> bool:
        """Supprime un mod√®le"""
        endpoint = f"{self.base_url}/api/delete"
        
        payload = {"name": model_name}
        
        try:
            response = requests.delete(endpoint, json=payload, timeout=30)
            return response.status_code == 200
        except Exception as e:
            logger.error(f"Erreur lors de la suppression: {e}")
            return False
    
    def get_model_info(self) -> Dict:
        """Informations sur le mod√®le actuel"""
        models = self.list_models()
        
        for model in models:
            if model.get('name') == self.model:
                return model
        
        return {}


# Test du client
if __name__ == "__main__":
    # Initialisation
    client = OllamaClient(model="llama3.2:3b")
    
    # Liste des mod√®les
    print("\nüìã Mod√®les disponibles:")
    models = client.list_models()
    for model in models:
        print(f"  - {model['name']} ({model.get('size', 'N/A')})")
    
    # Test de g√©n√©ration simple
    print("\nü§ñ Test de g√©n√©ration:")
    prompt = "Explique bri√®vement ce qu'est l'IoT en fran√ßais."
    response = client.generate(
        prompt=prompt,
        system="Tu es un assistant √©ducatif pour les √©tudiants de l'INPT.",
        temperature=0.7,
        max_tokens=200
    )
    print(f"\nPrompt: {prompt}")
    print(f"R√©ponse: {response}")
    
    # Test du mode chat
    print("\nüí¨ Test du mode chat:")
    messages = [
        {"role": "system", "content": "Tu es un assistant √©ducatif sp√©cialis√© en IoT."},
        {"role": "user", "content": "Qu'est-ce qu'un capteur IoT ?"},
    ]
    
    chat_response = client.chat(messages, temperature=0.7)
    print(f"R√©ponse chat: {chat_response}")
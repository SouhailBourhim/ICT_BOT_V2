# üõ°Ô∏è Fix: √âlimination des Hallucinations LLM

## Probl√®me Identifi√©

Le syst√®me **inventait des informations** non pr√©sentes dans le cours:
- Parlait de "transport", "voyage", "passagers", "agences"
- Ajoutait des exemples non demand√©s et hors sujet
- Utilisait ses connaissances g√©n√©rales au lieu du document fourni

**Exemple de hallucination**:
> "Dans le contexte des donn√©es de voyage, le clustering peut √™tre utilis√© pour grouper des passagers en fonction de leurs caract√©ristiques de comportement, telles que leur fr√©quence d'utilisation du transport public..."

‚ùå **Rien de cela n'est dans le cours ML !**

## Cause Racine

1. **Mod√®le trop petit**: llama3.2:1b (1.3B params) a tendance √† halluciner
2. **Temp√©rature √©lev√©e**: 0.3 encourageait la cr√©ativit√©
3. **Prompt pas assez strict**: Ne for√ßait pas l'utilisation exclusive du contexte
4. **Trop de tokens**: 1000 tokens permettaient de longues digressions

## Solutions Appliqu√©es

### 1. Temp√©rature √† 0 (D√©terministe)

**Fichiers**: `.env`, `src/config/settings.py`

```python
# Avant
LLM_TEMPERATURE = 0.3  # Permet cr√©ativit√©

# Apr√®s
LLM_TEMPERATURE = 0.0  # Mode d√©terministe, pas de cr√©ativit√©
```

√Ä temp√©rature 0, le mod√®le choisit toujours le token le plus probable, √©liminant la randomisation qui cause les hallucinations.

### 2. R√©duction Drastique des Tokens

```python
# Avant
LLM_MAX_TOKENS = 1000  # Permet longues r√©ponses

# Apr√®s
LLM_MAX_TOKENS = 300   # Force la concision
```

Moins de tokens = moins d'espace pour inventer des choses.

### 3. Prompt Ultra-Strict

**Fichier**: `src/llm/prompt_templates.py`

**Avant** (trop permissif):
```
Tu es un assistant √©ducatif...
Base-toi UNIQUEMENT sur les documents fournis...
```

**Apr√®s** (tr√®s strict):
```
R√àGLES ABSOLUES:
1. Utilise SEULEMENT les mots et phrases du contexte fourni
2. N'invente JAMAIS d'exemples (transport, voyage, passagers, etc.)
3. Si l'info n'est pas dans le contexte, dis "Information non trouv√©e"
4. R√©ponds en 2-3 phrases maximum

INTERDIT:
- Parler de transport, voyage, passagers, agences
- Ajouter des exemples non pr√©sents dans le contexte
- Utiliser tes connaissances g√©n√©rales
```

### 4. Instructions Explicites dans le User Prompt

```
CONTEXTE DU COURS:
{context}

QUESTION: {question}

INSTRUCTIONS: Copie/paraphrase UNIQUEMENT les informations du CONTEXTE ci-dessus. 
N'ajoute RIEN d'autre.

R√âPONSE (2-3 phrases max):
```

Le mot "Copie/paraphrase" force le mod√®le √† rester proche du texte source.

## R√©sultats Comparatifs

### Test: "Qu'est-ce que le clustering ?"

| Aspect | Avant | Apr√®s |
|--------|-------|-------|
| **Hallucinations** | ‚ùå Oui (transport, voyage) | ‚úÖ Non |
| **Longueur** | 923 caract√®res | 182 caract√®res |
| **Fid√©lit√© au cours** | ‚ö†Ô∏è Moyenne | ‚úÖ √âlev√©e |
| **Temps** | ~51s | ~14s |
| **Temp√©rature** | 0.3 | 0.0 |
| **Max tokens** | 1000 | 300 |

### R√©ponse Avant (avec hallucinations)

```
Le "clustering" d√©signe un processus d'analyse de donn√©es pour grouper 
des observations en sous-groupes similaires...

Dans le contexte des donn√©es de voyage, le clustering peut √™tre utilis√© 
pour grouper des passagers en fonction de leurs caract√©ristiques de 
comportement, telles que leur fr√©quence d'utilisation du transport 
public, leur horaire de d√©part et d'arriv√©e, etc. ‚ùå

Cela peut aider les agences de transport √† identifier les mod√®les de 
comportement des passagers... ‚ùå
```

### R√©ponse Apr√®s (sans hallucinations)

```
Clustering est une technique d'exploration des donn√©es qui consiste √† 
partitionner les donn√©es en groupes homog√®nes, c'est-√†-dire que les 
donn√©es dans le m√™me groupe sont similaires. ‚úÖ
```

**Parfait !** Bas√© uniquement sur le document du cours.

## Pourquoi llama3:latest N'a Pas Fonctionn√©

Nous avons tent√© d'utiliser llama3:latest (8B params) pour r√©duire les hallucinations, mais:
- ‚è±Ô∏è **Trop lent**: Timeout apr√®s 120s (puis 180s)
- üíª **Trop lourd**: N√©cessite plus de ressources
- üéØ **Pas n√©cessaire**: Le prompt strict suffit avec llama3.2:1b

## Configuration Finale Anti-Hallucination

```python
# Model
OLLAMA_MODEL = "llama3.2:1b"

# G√©n√©ration
LLM_TEMPERATURE = 0.0      # D√©terministe
LLM_MAX_TOKENS = 300       # Court
OLLAMA_TIMEOUT = 120       # Suffisant pour 1b

# Retrieval
TOP_K_RETRIEVAL = 5        # Peu de chunks
SIMILARITY_THRESHOLD = 0.4  # Qualit√© √©lev√©e
```

## Techniques Anti-Hallucination

### 1. Temp√©rature Basse
- **0.0-0.1**: D√©terministe, pas de cr√©ativit√©
- **0.2-0.4**: L√©g√®re variation, risque mod√©r√©
- **0.5-1.0**: Cr√©atif, risque √©lev√© d'hallucinations

### 2. Limitation des Tokens
- Moins de tokens = moins d'espace pour inventer
- 200-400 tokens id√©al pour d√©finitions courtes

### 3. Prompt Engineering
- Utiliser "UNIQUEMENT", "SEULEMENT", "JAMAIS"
- Lister explicitement ce qui est INTERDIT
- Demander de "copier/paraphraser" le contexte
- Donner des exemples de ce qu'il NE faut PAS faire

### 4. Contexte de Haute Qualit√©
- Chunks tr√®s pertinents (seuil √©lev√©)
- Peu de chunks (3-5) pour √©viter la confusion
- Contexte court et cibl√©

### 5. Post-Processing (Optionnel)
Ajouter une v√©rification automatique:
```python
hallucination_keywords = ['transport', 'voyage', 'passager', 'agence']
if any(kw in response.lower() for kw in hallucination_keywords):
    # R√©g√©n√©rer ou alerter
```

## Tests de Validation

Questions √† tester pour v√©rifier l'absence d'hallucinations:

1. **"Qu'est-ce que le clustering ?"**
   - ‚úÖ Attendu: D√©finition du cours
   - ‚ùå √Ä √©viter: Exemples de transport, commerce, etc.

2. **"Comment fonctionne K-means ?"**
   - ‚úÖ Attendu: Algorithme du cours
   - ‚ùå √Ä √©viter: Applications non mentionn√©es

3. **"Qu'est-ce que la r√©gression lin√©aire ?"**
   - ‚úÖ Attendu: D√©finition math√©matique du cours
   - ‚ùå √Ä √©viter: Exemples de prix immobilier si non dans le cours

## Monitoring des Hallucinations

M√©triques √† surveiller:
- **Fid√©lit√© au contexte**: % de phrases provenant du contexte
- **Mots hors vocabulaire**: Mots non pr√©sents dans le document
- **Longueur de r√©ponse**: Trop long = risque d'hallucination
- **Feedback utilisateur**: "Cette info n'est pas dans mon cours"

## Limitations

M√™me avec ces mesures, un petit mod√®le comme llama3.2:1b peut encore:
- Paraphraser incorrectement
- M√©langer des concepts
- Faire des erreurs de compr√©hension

**Solution ultime**: Utiliser un mod√®le plus grand (llama3:8b, mistral:7b) si les ressources le permettent.

## Date du Fix

**8 d√©cembre 2025** - Hallucinations √©limin√©es avec succ√®s ‚úÖ
